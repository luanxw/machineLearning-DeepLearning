## 深度学习

**传统机器学习:**

 1、特征提取与人工整理

 2、选择机器学习模型

3、针对训练集对模型进行训练,得到最优的模型参数

4、针对测试集进行测试、得到测试结果

**深度学习(以图像学习为例):**

1、输入(图像)

2、提取特征值(灰度处理、整理基础图片特征数据、图像像素)

3、提取复杂特征(可以有多个步骤、每次提取不同的图像特征:eg: 线条、简单形状、复杂形状)

4、模型训练、得到优化模型参数

5、得到预测结果

**人工神经网络:**

感知机模型:

 为什么增加一个非线性函数?

不恰当的例子: 就像你有个地方痛、每次都是线性求和传输、叠加到最后是一个很大的值

简单神经网络可以分为三层: 输入层、隐藏层、输出层

设计一个神经元时,输入层和输出层的节点数往往是固定的、中间的隐藏层可以自由指定

神经网络结构图中的拓扑和箭头代表预测过程中的着数据流向、跟训练时数据流向有一定区别

结构图里关键不是圆圈(代表“神经元”)、而是连接线(代表:“神经元之间的连接”)、每个连接线代表不同的权重(其值称为权值)、这是训练得到的

神经网络训练过程

1、整理输入与输出

有一组样本数据、每个样本有三个特征值输入与一个输出结果、我们需要做的就是根据三个输入特征值预测输出

2、模型搭建与训练

依据设计好的神经网络结构、为每一层输入分配权重、完成神经网络的正向搭建、基于正向传播计算样本预测输出、根据已知训练样本、设计损失函数、基于反响传播不断迭代每一层的权重参数使得损失函数向最低点快速收敛

3、预测

使用训练好的一组权重、对未知输入进行结果预测

GPU擅长: 矩阵运算、图片处理

#### TensorFlow

            TensorFlow是一个采用数据流程图(data flow graphs)、用于数值计算的开源库. 节点(nodes)在图中表述数学操作、图中的线(edges)则表示节点间相互联系的多维数据数组、即张量(tensor、在Tensor Flow中可以简单理解过多维数组). 它灵活的架构让你可以在多种平台上展开计算. 例如: 一台服务器上一个或者多个CPU(或者GPU)、移动设备

数据流程图用“节点”(nodes)和“线”(edges)的有向图来描述数学计算. “节点”一般用来表示施加的数学操作、但也可以表述数据输入(feed in)的起点/输出(push out)的终点、或者是读取/写入持久变量( persistent variable)的终点. “线”表示“节点”间的输入输出关系.这些线“线”可以运输“size可动态调整”的多维数组,即张量(tensor). 张量从图中流过的直观图像是这个工具被命名为“TensorFlow的原因. 一旦输入端的所有张量准备好,节点将被分配到各种计算设备中完成异步并行地执行运算

            TensorFlow是一个通过计算图的形式来表述计算的变成系统. TensorFlow中每一个计算都说图中的一个节点、而节点之间的变描述了计算之间的依赖关系

                如果说TensorFlow中、Tensor表明了数据结构、那么flow则体现了他的计算模型.

TensorFlow 只是保存执行过程、执行顺序保存下来、在调用run方法时,一下提交给cup,

tensorflow.**Tensor** 

tensorflow中并不是之间采用结果计算、而是对计算结果的引用. 在张量中并没有保存真正的数字、而是保存如何得到这些数字的过程

```python
import tensorflow 

a = tensorflow.constant([1.0,2.0],name='a')
b = tensorflow.constant([2.0,3.0],name='b')
result = tensorflow.add(a,b,name='add')

sess = tensorflow.session()
result = sess.run(result)
print(result, type(result))
sess.close()


with tensorflow.session() as sess:
       print(sess.run(result * 2)) # 使用with不需要使用close(),系统自动添加
```

###### numpy数据类型对象dtype

```gcode
名称                 描述
np.bool_    布尔型数据类型
np.int_    默认的整数类型
np.intc    与 C 的 int 类型一样，一般是 int32 或 int 64
np.intp    用于索引的整数类型，一般是 int32 或 int64
np.int8    8位整数即1字节（-128 to 127）
np.int16    16位整数（-32768 to 32767）
np.int32    32位整数（-2147483648 to 2147483647）
np.int64    64位整数（-9223372036854775808 to 9223372036854775807）
np.uint8    8位无符号整数（0 to 255）
np.uint16    16位无符号整数（0 to 65535）
np.uint32    32位无符号整数（0 to 4294967295）
np.uint64    64位无符号整数（0 to 18446744073709551615）
np.float_    float64 简写，即64位双精度浮点数
np.float16    16位半精度浮点数，包括：1 个符号位，5 个指数位，10 个尾数位
np.float32    32位 单精度浮点数，包括：1 个符号位，8 个指数位，23 个尾数位
np.float64    64位双精度浮点数，包括：1 个符号位，11 个指数位，52 个尾数位
np.complex_    complex128 简写，即 128 位复数
np.complex64    复数，表示双 32 位浮点数（实数部分和虚数部分）
np.complex128    复数，表示双 64 位浮点数（实数部分和虚数部分）
```

###### TensorFlow数据类型对象Dtype

```gcode
名称      描述
tf.float16    16位半精度浮点
tf.float32    32位单精度浮点
tf.float64    64位双精度浮点
tf.bfloat16    16位截断浮点
tf.complex64    64位单精度复数
tf.complex128    128位双精度复数
tf.int8    8位有符号整数
tf.uint8    8位无符号整数
tf.uint16    16位无符号整数
tf.int16    16位有符号整数
tf.int32    32位有符号整数
tf.int64    64位有符号整数
tf.bool    布尔值
tf.string    字符串
tf.qint8    量化的8位带符号整数
tf.quint8    量化的8位无符号整数
tf.qint16    量化的16位有符号整数
tf.quint16    量化的16位无符号整数
tf.qint32    量化的32位有符号整数
```

#### 损失函数

损失函数（loss function），量化了分类器输出的结果（预测值）和咱们指望的结果（标签）之间的差距，这和分类器结构自己一样重要。

##### 损失函数--均方误差MSE(用于线性回归问题)

其实就是各个预测值与实际值的差平方的均值

用tensorflow函数表述为:

```python
loss_mse =tensorflow.reduce_mean(tensorflow.square(y - y_))
```

##### 损失函数--交叉熵(用于分类问题)

##### 反向传播训练方法

以减小loss值为优化目标、有梯度下降、adam优化器等等优化方法

#### 神经网络计算过程

1、导入模块, 生成模拟数据集

    import 

    常量定义

    生成数据集

2、前向传播: 定于输入、参数和输出

    x=   (训练数据)        y=    (真实输出)

    w1=       w2=     (参数)

    b=   (y= w1x+w2x+b中的阈值)        y_=      (预测输出)

3、反向传播: 定义损失函数、反向传播方法

    loss=

    train_step= 

4、生成会话、训练steps轮(tensor中、不再需要session)

#### 卷积层

        神经网络中、卷积层就是卷积运算对原始图像或者上一层的特征进行变幻的层. 特定的卷积内核可以对图像进行一种特定变换、从而提取某种特定的特征,如:横向边缘和竖向边缘.

        在一个卷积层中、为了从图像中提取更多种形式的特征、我们通常使用多个卷积内核对输入图像做不同的卷积操作、一个卷积可以得到一个通道为1的三阶张量、多个卷积就可以得到多个通道为1的三阶张量、这个三阶张量的通道数就等于我们使用的卷积核的个数、由于每一个通道中提取一种特征、我们也将这个三阶张量称为**特征图**(*feature map*)

        特征图与彩色图像都是三阶张量、也都有若干个通道、因此卷积层不仅可以作用于图像、也可以作用于其他图像输出的特征图、通常一个深度学习网络的第一个卷积层会以图像作为输入、而后的卷积层会以前面的特征图作为输入

#### 非线性激活层

    通常我们需要在卷积层和全连接层后面都连接一个非线性激活层( non-linear activation layer). 为什么呢? 不管是卷积层、还是全链接层中的运算、他们都是自变量中的一次函数、即所谓的线性函数(linear function) . 线性函数有一个性质: 若干线性计算的结果仍是线性的.    换句话说、如果我们直接将奖卷积层和全链接层直接堆接起来、那么对图像产生的效果就可以被一个全链接层代替. 这样一来、虽然我们堆叠了很多层、但对每一层的变换结果实际上被合并掉了一起; 而如果每次线性运算后、再进行一次非线性运算、那么每次变换的结果就可以保留. 

###### 常见的非线性激活函数:

1、**逻辑函数(logistics function):  sigmoid**

###### f(x)= $\frac{1}{1+e^{-z}}$

![](/Users/allen/Library/Application%20Support/marktext/images/2022-03-13-22-26-27-image.png)

2、**双曲正切函数(hyperbolic tangent function ):  简称  tanh**

        双曲函数类似于常见的(也叫圆函数的)三角函数。基本双曲函数是双曲[正弦](http://baike.so.com/doc/5765956-5978724.html)"sinh"，双曲余弦"cosh"，从它们导出双曲[正切](http://baike.so.com/doc/6760450-6975091.html)"tanh"

![](/Users/allen/Library/Application%20Support/marktext/images/2022-03-13-22-32-34-image.png)

        **sigmoid和tanh激活函数有共同的缺点：即在z很大或很小时，梯度几乎为零，因此使用梯度下降优化算法更新网络很慢**。

3、**修正线性函数(rectified linear function): 简称  relu**

![](/Users/allen/Library/Application%20Support/marktext/images/2022-03-13-22-41-38-image.png)

![](/Users/allen/Library/Application%20Support/marktext/images/2022-03-13-22-45-05-image.png)

##### 各个损失函数的比较:

**为什么通常Relu比sigmoid和tanh强，有什么不同？**  
　　主要是因为它们gradient特性不同。sigmoid和tanh的gradient在饱和区域非常平缓，接近于0，很容易造成vanishing gradient的问题，减缓收敛速度。vanishing gradient在网络层数多的时候尤其明显，是加深网络结构的主要障碍之一。相反，Relu的gradient大多数情况下是常数，有助于解决深层网络的收敛问题。Relu的另一个优势是在生物上的合理性，它是单边的，相比sigmoid和tanh，更符合生物神经元的特征。  
　　而提出sigmoid和tanh，主要是因为它们全程可导。还有表达区间问题，sigmoid和tanh区间是0到1，或着-1到1，在表达上，尤其是输出层的表达上有优势。

　　ReLU更容易学习优化。因为其分段线性性质，导致其前传，后传，求导都是分段线性。而传统的sigmoid函数，由于两端饱和，在传播过程中容易丢弃信息：

**为什么引入Relu呢？**  
         第一，采用sigmoid等函数，*算激活函数时（指数运算），计算量大*，反向传播求误差梯度时，求导涉及除法，计算量相对大，而采用Relu激活函数，整个过程的计算量节省很多。  
         第二，对于深层网络，sigmoid函数反向传播时，很容易就会出现梯度消失的情况（在sigmoid接近饱和区时，变换太缓慢，导数趋于0，这种情况会造成*信息丢失，从而无法完成深层网络的训练。  
         第三，Relu会使一部分神经元的输出为0，这样就造成了网络的稀疏性，并且减少了参数的相互依存关系，缓解了过拟合问题的发生（以及一些人的生物解释balabala）。  

         当然现在也有一些对relu的改进，比如prelu，random relu等，在不同的数据集上会有一些训练速度上或者准确率上的改进，具体的大家可以找相关的paper看。  
         多加一句，现在主流的做法，会在做完relu之后，加一步batch normalization，尽可能保证每一层网络的输入具有相同的分布[1]。而最新的paper[2]，他们在加入bypass connection之后，发现改变batch normalization的位置会有更好的效果。大家有兴趣可以看一下. 

#### 池化层(pooling layer) :

        在计算卷积的时候、我们会用卷积核滑过图像或者特征图的每一个像素. 如果图像或者特征图的分辨率很高、那么卷积的计算量就很大; 为了解决这个问题、我们通常在几个卷积层之后插入池化、以降低特征图的分辨率

        池化层一般操作如下:首先将特征图按照通道分开、得到若干个矩阵、对于每个矩阵、将其切割组成若干大小相等的正方形小块矩阵、然后对每小块矩阵取最大值或者平均值、并将结果组成一个新的矩阵、最后将所有的通道结果矩阵、按照原顺序堆叠起来组成一个三阶张量、这个三阶张量就是池化层的输出. **对于每一个区块取最大值池化、称之为最大池化层、取平均值的池化层、称之为平均池化层**

##### 全链接层(FC full connection) :

        在整个卷积网络层中起到“分类器”的作用、即:经过卷积、激活函数、池化等深度网络后、再经过全链接层对结果进行识别分类 

    由于神经网络属于监督学习、在模型训练时、根据训练样本对模型进行训练、从而得到全连链接层对权重



<img src="file:///Users/allen/Library/Application%20Support/marktext/images/2022-03-14-22-53-49-image.png" title="" alt="" width="679">
