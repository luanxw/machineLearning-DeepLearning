## 深度学习

**传统机器学习:**

 1、特征提取与人工整理

 2、选择机器学习模型

3、针对训练集对模型进行训练,得到最优的模型参数

4、针对测试集进行测试、得到测试结果

**深度学习(以图像学习为例):**

1、输入(图像)

2、提取特征值(灰度处理、整理基础图片特征数据、图像像素)

3、提取复杂特征(可以有多个步骤、每次提取不同的图像特征:eg: 线条、简单形状、复杂形状)

4、模型训练、得到优化模型参数

5、得到预测结果

**人工神经网络:**

感知机模型:

 为什么增加一个非线性函数?

不恰当的例子: 就像你有个地方痛、每次都是线性求和传输、叠加到最后是一个很大的值

简单神经网络可以分为三层: 输入层、隐藏层、输出层

设计一个神经元时,输入层和输出层的节点数往往是固定的、中间的隐藏层可以自由指定

神经网络结构图中的拓扑和箭头代表预测过程中的着数据流向、跟训练时数据流向有一定区别

结构图里关键不是圆圈(代表“神经元”)、而是连接线(代表:“神经元之间的连接”)、每个连接线代表不同的权重(其值称为权值)、这是训练得到的

神经网络训练过程

1、整理输入与输出

有一组样本数据、每个样本有三个特征值输入与一个输出结果、我们需要做的就是根据三个输入特征值预测输出

2、模型搭建与训练

依据设计好的神经网络结构、为每一层输入分配权重、完成神经网络的正向搭建、基于正向传播计算样本预测输出、根据已知训练样本、设计损失函数、基于反响传播不断迭代每一层的权重参数使得损失函数向最低点快速收敛

3、预测

使用训练好的一组权重、对未知输入进行结果预测

GPU擅长: 矩阵运算、图片处理

#### softmax

在[机器学习](https://so.csdn.net/so/search?q=%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0&spm=1001.2101.3001.7020)尤其是深度学习中,softmax是个非常常用而且比较重要的函数，尤其在多分类的场景中使用广泛。

- 它把一些输入映射为0-1之间的实数，

- 归一化保证所有输出和为1，多分类的概率之和也刚好为1。
  
  在 Softmax 函数中，在分母中，它取所有指数值的总和来归一化所有类的值。它考虑了范围内所有类的值，我们通常在最后一层使用它。要通过分析所有的值来知道Input属于哪个类，。

#### TensorFlow

            TensorFlow是一个采用数据流程图(data flow graphs)、用于数值计算的开源库. 节点(nodes)在图中表述数学操作、图中的线(edges)则表示节点间相互联系的多维数据数组、即张量(tensor、在Tensor Flow中可以简单理解过多维数组). 它灵活的架构让你可以在多种平台上展开计算. 例如: 一台服务器上一个或者多个CPU(或者GPU)、移动设备

数据流程图用“节点”(nodes)和“线”(edges)的有向图来描述数学计算. “节点”一般用来表示施加的数学操作、但也可以表述数据输入(feed in)的起点/输出(push out)的终点、或者是读取/写入持久变量( persistent variable)的终点. “线”表示“节点”间的输入输出关系.这些线“线”可以运输“size可动态调整”的多维数组,即张量(tensor). 张量从图中流过的直观图像是这个工具被命名为“TensorFlow的原因. 一旦输入端的所有张量准备好,节点将被分配到各种计算设备中完成异步并行地执行运算

            TensorFlow是一个通过计算图的形式来表述计算的变成系统. TensorFlow中每一个计算都说图中的一个节点、而节点之间的变描述了计算之间的依赖关系

                如果说TensorFlow中、Tensor表明了数据结构、那么flow则体现了他的计算模型.

TensorFlow 只是保存执行过程、执行顺序保存下来、在调用run方法时,一下提交给cup,

tensorflow.**Tensor** 

tensorflow中并不是之间采用结果计算、而是对计算结果的引用. 在张量中并没有保存真正的数字、而是保存如何得到这些数字的过程

```python
import tensorflow 

a = tensorflow.constant([1.0,2.0],name='a')
b = tensorflow.constant([2.0,3.0],name='b')
result = tensorflow.add(a,b,name='add')

sess = tensorflow.session()
result = sess.run(result)
print(result, type(result))
sess.close()


with tensorflow.session() as sess:
       print(sess.run(result * 2)) # 使用with不需要使用close(),系统自动添加
```

###### numpy数据类型对象dtype

```gcode
名称                 描述
np.bool_    布尔型数据类型
np.int_    默认的整数类型
np.intc    与 C 的 int 类型一样，一般是 int32 或 int 64
np.intp    用于索引的整数类型，一般是 int32 或 int64
np.int8    8位整数即1字节（-128 to 127）
np.int16    16位整数（-32768 to 32767）
np.int32    32位整数（-2147483648 to 2147483647）
np.int64    64位整数（-9223372036854775808 to 9223372036854775807）
np.uint8    8位无符号整数（0 to 255）
np.uint16    16位无符号整数（0 to 65535）
np.uint32    32位无符号整数（0 to 4294967295）
np.uint64    64位无符号整数（0 to 18446744073709551615）
np.float_    float64 简写，即64位双精度浮点数
np.float16    16位半精度浮点数，包括：1 个符号位，5 个指数位，10 个尾数位
np.float32    32位 单精度浮点数，包括：1 个符号位，8 个指数位，23 个尾数位
np.float64    64位双精度浮点数，包括：1 个符号位，11 个指数位，52 个尾数位
np.complex_    complex128 简写，即 128 位复数
np.complex64    复数，表示双 32 位浮点数（实数部分和虚数部分）
np.complex128    复数，表示双 64 位浮点数（实数部分和虚数部分）
```

###### TensorFlow数据类型对象Dtype

```gcode
名称      描述
tf.float16    16位半精度浮点
tf.float32    32位单精度浮点
tf.float64    64位双精度浮点
tf.bfloat16    16位截断浮点
tf.complex64    64位单精度复数
tf.complex128    128位双精度复数
tf.int8    8位有符号整数
tf.uint8    8位无符号整数
tf.uint16    16位无符号整数
tf.int16    16位有符号整数
tf.int32    32位有符号整数
tf.int64    64位有符号整数
tf.bool    布尔值
tf.string    字符串
tf.qint8    量化的8位带符号整数
tf.quint8    量化的8位无符号整数
tf.qint16    量化的16位有符号整数
tf.quint16    量化的16位无符号整数
tf.qint32    量化的32位有符号整数
```

#### 损失函数

损失函数（loss function），量化了分类器输出的结果（预测值）和咱们指望的结果（标签）之间的差距，这和分类器结构自己一样重要。

##### 损失函数--均方误差MSE(用于线性回归问题)

其实就是各个预测值与实际值的差平方的均值

用tensorflow函数表述为:

```python
loss_mse =tensorflow.reduce_mean(tensorflow.square(y - y_))
```

##### 损失函数--交叉熵(用于分类问题)

##### 反向传播训练方法

以减小loss值为优化目标、有梯度下降、adam优化器等等优化方法

#### 神经网络计算过程

1、导入模块, 生成模拟数据集

    import 

    常量定义

    生成数据集

2、前向传播: 定于输入、参数和输出

    x=   (训练数据)        y=    (真实输出)

    w1=       w2=     (参数)

    b=   (y= w1x+w2x+b中的阈值)        y_=      (预测输出)

3、反向传播: 定义损失函数、反向传播方法

    loss=

    train_step= 

4、生成会话、训练steps轮(tensor中、不再需要session)

#### 卷积层

        神经网络中、卷积层就是卷积运算对原始图像或者上一层的特征进行变幻的层. 特定的卷积内核可以对图像进行一种特定变换、从而提取某种特定的特征,如:横向边缘和竖向边缘.

        在一个卷积层中、为了从图像中提取更多种形式的特征、我们通常使用多个卷积内核对输入图像做不同的卷积操作、一个卷积可以得到一个通道为1的三阶张量、多个卷积就可以得到多个通道为1的三阶张量、这个三阶张量的通道数就等于我们使用的卷积核的个数、由于每一个通道中提取一种特征、我们也将这个三阶张量称为**特征图**(*feature map*)

        特征图与彩色图像都是三阶张量、也都有若干个通道、因此卷积层不仅可以作用于图像、也可以作用于其他图像输出的特征图、通常一个深度学习网络的第一个卷积层会以图像作为输入、而后的卷积层会以前面的特征图作为输入

#### 非线性激活层

    通常我们需要在卷积层和全连接层后面都连接一个非线性激活层( non-linear activation layer). 为什么呢? 不管是卷积层、还是全链接层中的运算、他们都是自变量中的一次函数、即所谓的线性函数(linear function) . 线性函数有一个性质: 若干线性计算的结果仍是线性的.    换句话说、如果我们直接将奖卷积层和全链接层直接堆接起来、那么对图像产生的效果就可以被一个全链接层代替. 这样一来、虽然我们堆叠了很多层、但对每一层的变换结果实际上被合并掉了一起; 而如果每次线性运算后、再进行一次非线性运算、那么每次变换的结果就可以保留. 

###### 常见的非线性激活函数:

1、**逻辑函数(logistics function):  sigmoid**

###### f(x)= $\frac{1}{1+e^{-z}}$

![](/Users/allen/Library/Application%20Support/marktext/images/2022-03-13-22-26-27-image.png)

2、**双曲正切函数(hyperbolic tangent function ):  简称  tanh**

        双曲函数类似于常见的(也叫圆函数的)三角函数。基本双曲函数是双曲[正弦](http://baike.so.com/doc/5765956-5978724.html)"sinh"，双曲余弦"cosh"，从它们导出双曲[正切](http://baike.so.com/doc/6760450-6975091.html)"tanh"

![](/Users/allen/Library/Application%20Support/marktext/images/2022-03-13-22-32-34-image.png)

        **sigmoid和tanh激活函数有共同的缺点：即在z很大或很小时，梯度几乎为零，因此使用梯度下降优化算法更新网络很慢**。

3、**修正线性函数(rectified linear function): 简称  relu**

![](/Users/allen/Library/Application%20Support/marktext/images/2022-03-13-22-41-38-image.png)

![](/Users/allen/Library/Application%20Support/marktext/images/2022-03-13-22-45-05-image.png)

##### 各个损失函数的比较:

**为什么通常Relu比sigmoid和tanh强，有什么不同？**  
　　主要是因为它们gradient特性不同。sigmoid和tanh的gradient在饱和区域非常平缓，接近于0，很容易造成vanishing gradient的问题，减缓收敛速度。vanishing gradient在网络层数多的时候尤其明显，是加深网络结构的主要障碍之一。相反，Relu的gradient大多数情况下是常数，有助于解决深层网络的收敛问题。Relu的另一个优势是在生物上的合理性，它是单边的，相比sigmoid和tanh，更符合生物神经元的特征。  
　　而提出sigmoid和tanh，主要是因为它们全程可导。还有表达区间问题，sigmoid和tanh区间是0到1，或着-1到1，在表达上，尤其是输出层的表达上有优势。

　　ReLU更容易学习优化。因为其分段线性性质，导致其前传，后传，求导都是分段线性。而传统的sigmoid函数，由于两端饱和，在传播过程中容易丢弃信息：

**为什么引入Relu呢？**  
         第一，采用sigmoid等函数，*算激活函数时（指数运算），计算量大*，反向传播求误差梯度时，求导涉及除法，计算量相对大，而采用Relu激活函数，整个过程的计算量节省很多。  
         第二，对于深层网络，sigmoid函数反向传播时，很容易就会出现梯度消失的情况（在sigmoid接近饱和区时，变换太缓慢，导数趋于0，这种情况会造成*信息丢失，从而无法完成深层网络的训练。  
         第三，Relu会使一部分神经元的输出为0，这样就造成了网络的稀疏性，并且减少了参数的相互依存关系，缓解了过拟合问题的发生（以及一些人的生物解释balabala）。  

         当然现在也有一些对relu的改进，比如prelu，random relu等，在不同的数据集上会有一些训练速度上或者准确率上的改进，具体的大家可以找相关的paper看。  
         多加一句，现在主流的做法，会在做完relu之后，加一步batch normalization，尽可能保证每一层网络的输入具有相同的分布[1]。而最新的paper[2]，他们在加入bypass connection之后，发现改变batch normalization的位置会有更好的效果。大家有兴趣可以看一下. 

#### 池化层(pooling layer) :

        在计算卷积的时候、我们会用卷积核滑过图像或者特征图的每一个像素. 如果图像或者特征图的分辨率很高、那么卷积的计算量就很大; 为了解决这个问题、我们通常在几个卷积层之后插入池化、以降低特征图的分辨率

        池化层一般操作如下:首先将特征图按照通道分开、得到若干个矩阵、对于每个矩阵、将其切割组成若干大小相等的正方形小块矩阵、然后对每小块矩阵取最大值或者平均值、并将结果组成一个新的矩阵、最后将所有的通道结果矩阵、按照原顺序堆叠起来组成一个三阶张量、这个三阶张量就是池化层的输出. **对于每一个区块取最大值池化、称之为最大池化层、取平均值的池化层、称之为平均池化层**

##### 全链接层(FC full connection) :

        在整个卷积网络层中起到“分类器”的作用、即:经过卷积、激活函数、池化等深度网络后、再经过全链接层对结果进行识别分类 

    由于神经网络属于监督学习、在模型训练时、根据训练样本对模型进行训练、从而得到全连链接层对权重

<img src="file:///Users/allen/Library/Application%20Support/marktext/images/2022-03-14-22-53-49-image.png" title="" alt="" width="679">

#### Convolutional Ceural Networks(卷积神经网络 CNN)

   背景: 图片是一个三维通道的tensor、**n** x**n** x **3**

通过特定的卷积内核去初步相乘去转化一个图形

##### CNN缺点:

    **data augmentation:** cnn在图像缩小、放大、旋转后无法识别

#### Spatial transformer(空间变换网络)

### pytorch

##### 在torch.nn下包含了我们模型的概念、一些常用层、损失函数等的定义。

##### torch.nn.Conv2d()

官网链接: https://pytorch.org/docs/1.11/generated/torch.nn.Conv2d.html#torch.nn.Conv2d

Conv2d，就是用来实现2d卷积操作的

in_channels —— 输入的channels数
out_channels —— 输出的channels数
kernel_size ——卷积核的尺寸，可以是方形卷积核、也可以不是，下边example可以看到
stride —— 步长，用来控制卷积核移动间隔
padding ——输入边沿扩边操作
padding_mode ——扩边的方式
bias ——是否使用偏置(即out = wx+b中的b)
groups:   (卷积核个数)：通常来说，卷积个数唯一，但是对某些情况，可以设置范围在1 —— in_channels中数目的卷积核：
dilation —— 这个参数简单说，设定了取数之间的间隔，

![](/Users/allen/Library/Application%20Support/marktext/images/2022-04-04-12-53-15-image.png)

输出res:
[ batch_size,output, height_3, width_3 ]

| batch_size | ----             | ----                |
|:----------:|:----------------:|:-------------------:|
| batch_size | 一个batch中样例的个数，同上 | 2                   |
| output     | 输出的深度            | 8                   |
| height_3   | 卷积结果的高度          | h1-h2+1 = 7-2+1 = 6 |
| weight_3   | 卷积结果的宽度          | w1-w2+1 = 3-3+1 = 1 |

参考博文:

- https://zhuanlan.zhihu.com/p/156825903

- https://stats.stackexchange.com/questions/295397/what-is-the-difference-between-conv1d-and-conv2d

##### torch.nn.BatchNorm2d()

    bn层的位置位于卷积神经网络的卷积层之后，对数据进行归一化处理，避免数据在进行下一步处理（卷积或激活函数）时因数据过大造成过拟合，导致网络性能不稳定，bn的函数数学原理如下

    BatchNorm2d()内部的参数如下：

    输入为一个四维数据(N,C,H,W)，N-输入的batch size，C是输入的图像的通道数，(H,W）为输入的图像的尺寸。  
对于每一个输入特征通道，所有样本的特征图做归一化处理。

1.num_features：一般输入参数为batch_size*num_features*height*width，即为其中特征的数量

2.eps：分母中添加的一个值，目的是为了计算的稳定性(不会出现分母为0 的错误），默认为：1e-5

3.momentum：一个用于运行过程中均值和方差的一个估计参数（我的理解是一个稳定系数，类似于SGD中的momentum的系数）

4.affine：当设为true时，会给定可以学习的系数矩阵gamma和beta

#### torch.nn.MaxPool2d()

```python
class torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)
```

参数：

- kernel_size(int or tuple) - max pooling的窗口大小，

- stride(int or tuple, optional) - max pooling的窗口移动的步长。默认值是kernel_size

- padding(int or tuple, optional) - 输入的每一条边补充0的层数

- dilation(int or tuple, optional) – 一个控制窗口中元素步幅的参数

- return_indices - 如果等于True，会返回输出最大值的序号，对于上采样操作会有帮助

- ceil_mode - 如果等于True，计算输出信号大小的时候，会使用向上取整，代替默认的向下取整的操作

![](https://upload-images.jianshu.io/upload_images/16715697-3d69c563031e9ec4.png)

### 

### 



### torch.nn.Dropout()

 随机将输入张量中部分元素设置为0。对于每次前向调用，被置0的元素都是随机的。

**参数：**

- **p** - 将元素置0的概率。默认值：0.5
- **in-place** - 若设置为True，会在原地执行操作。默认值：False

**形状：**

- **输入：** 任意。输入可以为任意形状。
- **输出：** 相同。输出和输入形状相同。

### torch.nn.GRU(*args,*kwargs)

中文自然语言处理--基于 Keras 的*GRU* 中文*文本分类*

    RU（Gate Recurrent Unit）是循环神经网络（Recurrent Neural Network, RNN）的一种。和LSTM（Long-Short Term Memory）一样，也是为了解决长期记忆和反向传播中的梯度等问题而提出来的。  
    相比LSTM，使用GRU能够达到相当的效果，并且相比之下更容易进行训练，能够很大程度上提高训练效率，因此很多时候会更倾向于使用GRU，其中GRU输入输出的结构与普通的RNN相似，其中的内部思想与LSTM相似

**input_size**：输入数据X的特征值的数目。

**hidden_size**：隐藏层的神经元数量，也就是隐藏层的特征数量。

**num_layers**：循环神经网络的层数，默认值是 1。

**bias**：默认为 True，如果为 false 则表示神经元不使用 ![[公式]](https://www.zhihu.com/equation?tex=bias_%7Bih%7D) 和 ![[公式]](https://www.zhihu.com/equation?tex=bias_%7Bhh%7D)偏移参数。

**batch_first**：如果设置为 True，则输入数据的维度中第一个维度就 是 batch 值，默认为 False。默认情况下第一个维度是序列的长度，第二个维度才是 - - batch，第三个维度是特征数目。

**dropout**：如果不为空，则表示最后跟一个 dropout 层抛弃部分数据，抛弃数据的比例由该参数指定。默认为0。

**bidirectional** : If True, becomes a bidirectional RNN. Default: False

**输入**：

**input**: [seq_len, batch, input_size]

![[公式]](https://www.zhihu.com/equation?tex=h_%7B0%7D) ​: [num_layers* num_directions, batch, hidden_size]

**输出**：

**output**: [seq_len, batch, num_directions * hidden_size]

![[公式]](https://www.zhihu.com/equation?tex=h_%7Bn%7D)​: [num_layers * num_directions, batch, hidden_size]

**参数**：

**GRU.weight_ih_l[k]**: 包括(W_ir|W_iz|W_in), 第0层[3*hidden_size, input_size]，之后为[3*hidden_size, num_directions * hidden_size]

**GRU.weight_hh_l[k]**: 包括(W_hr|W_hz|W_hn), [3*hidden_size, hidden_size]

**GRU.bias_ih_l[k]**: 包括(b_ir|b_iz|b_in), [3*hidden_size]

**GRU.bias_hh_l[k]**: 包括(b_hr|b_hz|b_hn), [3*hidden_size]

## Transformer

        Transformer是一个利用注意力机制来提高模型训练速度的模型。trasnformer可以说是完全基于自注意力机制的一个深度学习模型，因为它适用于并行化计算，和它本身模型的复杂程度导致它在精度和性能上都要高于之前流行的RNN循环神经网络。

    关于注意力机制可以参看[这篇文章](https://zhuanlan.zhihu.com/p/52119092)，

参考博文地址: https://zhuanlan.zhihu.com/p/82312421

那什么是transformer呢？

你可以简单理解为它是一个黑盒子，当我们在做文本翻译任务是，我输入进去一个中文，经过这个黑盒子之后，输出来翻译过后的英文

#### 自注意力机制(self-attention)

    self-attention的输入就是词向量，即整个模型的最初的输入是词向量的形式。那自注意力机制呢，顾名思义就是自己和自己计算一遍注意力，即对每一个输入的词向量

1. transformer首先将词向量乘上三个矩阵，得到三个新的向量，之所以乘上三个矩阵参数而不是直接用原本的词向量是因为这样增加更多的参数，提高模型效果。对于输入X1(机器)，乘上三个矩阵后分别得到Q1,K1,V1，同样的，对于输入X2(学习)，也乘上三个不同的矩阵得到Q2,K2,V2。

2. 那接下来就要计算注意力得分了，这个得分是通过计算Q与各个单词的K向量的点积得到的。我们以X1为例，分别将Q1和K1、K2进行点积运算，假设分别得到得分112和96。

3. 将得分分别除以一个特定数值8（K向量的维度的平方根，通常K向量的维度是64）这能让梯度更加稳定，则得到结果

4. 将上述结果进行softmax运算得到，softmax主要将分数标准化，使他们都是正数并且加起来等于1。

5. 将V向量乘上softmax的结果，这个思想主要是为了保持我们想要关注的单词的值不变，而掩盖掉那些不相关的单词（例如将他们乘上很小的数字）

6. 将带权重的各个V向量加起来，至此，产生在这个位置上（第一个单词）的self-attention层的输出，其余位置的self-attention输出也是同样的计算方式。
   
   **图片表示:**
   
   ![preview](https://pic4.zhimg.com/v2-0190eb46d1c46efc04926821e69fd377_r.jpg)
   
   **详细计算过程**
   
   ![](/Users/allen/Library/Application%20Support/marktext/images/2022-04-07-15-41-30-image.png)
   
   **一个b向量的计算过程**
   
   ![](/Users/allen/Library/Application%20Support/marktext/images/2022-04-07-15-42-05-image.png)

   **所有b向量的计算过程**![](/Users/allen/Library/Application%20Support/marktext/images/2022-04-07-15-41-46-image.png)

#### 多头注意力机制(multi-head attention)

进一步细化自注意力机制层，增加了“多头注意力机制”的概念，这从两个方面提高了自注意力层的性能。

1. 第一个方面，他扩展了模型关注不同位置的能力，这对翻译一下句子特别有用，比如: 我们想知道“it”是指代的哪个单词。

2. 第二个方面，他给了自注意力层多个“表示子空间”。对于多头自注意力机制，我们不止有一组Q/K/V权重矩阵，而是有多组（论文中使用8组），所以每个编码器/解码器使用8个“头”（可以理解为8个互不干扰自的注意力机制运算），每一组的Q/K/V都不相同。然后，得到8个不同的权重矩阵Z，每个权重矩阵被用来将输入向量投射到不同的表示子空间
   
       经过多头注意力机制后，就会得到多个权重矩阵Z，我们将多个Z进行拼接就得到了self-attention层的输出：
   
       上述我们经过了self-attention层，我们得到了self-attention的输出，self-attention的输出即是前馈神经网络层的输入，然后前馈神经网络的输入只需要一个矩阵就可以了，不需要八个矩阵，所以我们需要把这8个矩阵压缩成一个，我们怎么做呢？只需要把这些矩阵拼接起来然后用一个额外的权重矩阵与之相乘即可。

#### 前馈神经网络

关于前馈神经网络，网上已经有很多资料，在这里就不做过多讲解了，只需要知道，前馈神经网络的输入是self-attention的输出，即上图的Z,是一个矩阵，矩阵的维度是（序列长度×D词向量），之后前馈神经网络的输出也是同样的维度。

#### 编码器(encoder)

encoder是对输入（机器学习）进行编码，使用的是自注意力机制+前馈神经网络的结构

顺序为: self attention  -->  feed forward 

transformer中使用了6个encoder(即:这个过程重复了6次)，为了解决梯度消失的问题，在Encoders和Decoder中都是用了残差神经网络的结构，即每一个前馈神经网络的输入不光包含上述self-attention的输出Z，还包含最原始的输入。

#### 解码器(decoder)

和编码器结构类似

顺序为: self attention ---> attention --> feed forward

transformer最后的线性层接上一个softmax，其中线性层是一个简单的全连接神经网络，它将解码器产生的向量投影到一个更高维度的向量（logits）上，假设我们模型的词汇表是10000个词，那么logits就有10000个维度，每个维度对应一个惟一的词的得分。之后的softmax层将这些分数转换为概率。选择概率最大的维度，并对应地生成与之关联的单词作为此时间步的输出就是最终的输出啦！！

#### 位置编码

transformer是如何保证时序的?

        RNN中的每个输入是时序的，是又先后顺序的，但是Transformer整个框架下来并没有考虑顺序信息，这就需要提到另一个概念了：“位置编码”。

Tranformer中确实没有考虑顺序信息，可以在输入中做手脚，把输入变得有位置信息、给每个词向量加上一个有顺序特征的向量，发现sin和cos函数能够很好的表达这种特征，所以通常位置向量用以下公式来表示：

![preview](https://pic1.zhimg.com/v2-a671b951ef42d09c349db12c35175998_r.jpg)

最后祭出这张经典的图，最初看这张图的时候可能难以理解，希望大家在深入理解Transformer后再看这张图能够有更深刻的认识。

![](/Users/allen/Library/Application%20Support/marktext/images/2022-04-07-15-43-53-image.png)